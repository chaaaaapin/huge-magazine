---
title: "Nobody Likes Filling Out Forms. This Open-Source Project Wants You to Talk Instead."
date: "2026-02-19"
ph_rank: 6
ph_votes: 129
ph_comments: 13
ph_slug: "kollect-voice-agent"
ph_url: "https://www.producthunt.com/products/kollect-voice-agent?utm_campaign=producthunt-api&utm_medium=api-v2&utm_source=Application%3A+tdc-io2+%28ID%3A+275675%29"
product_url: "https://www.producthunt.com/r/RDNFXGXVMGMHF2?utm_campaign=producthunt-api&utm_medium=api-v2&utm_source=Application%3A+tdc-io2+%28ID%3A+275675%29"
logo: "https://ph-files.imgix.net/b730e1f6-48eb-464c-aeda-1cf520847863.png?auto=format"
hero_image: "https://ph-files.imgix.net/a5b71afb-e271-452b-8d69-f19ea4e43f27.png?auto=format"
topics:
  - "Open Source"
  - "Marketing"
  - "Artificial Intelligence"
tagline: "AI that feels human"
excerpt: "Kollect replaces the survey grid with a voice conversation — and it's self-hostable, MIT-licensed, and already making a case for why forms are a UX relic."
edition: "2026-02-19"
author: "jess-tran"
app_type: "WebApplication"
---

## The Macro: The Form Is a Design Failure We Just Stopped Noticing

Somewhere between the invention of the web form and the current era of AI that can hold a coherent conversation, we collectively decided that making people click through dropdowns and radio buttons was fine, actually. It is not fine. Completion rates on traditional surveys are notoriously bad — dropout tends to spike the moment a form feels long, which it always does, because grids of questions feel like homework.

The broader context here is that voice and conversational interfaces have been circling this problem for years without landing a clean solution. Typeform made forms feel less terrible through smart UX. Conversational survey tools like SurveyMonkey's conversational mode, or Voiceform (which specifically targets voice-first data collection), have taken runs at this. Deepgram — whose voice API Kollect's maker reportedly used to build the product — has made low-latency transcription accessible to indie developers in a way that simply wasn't possible three or four years ago. The infrastructure got cheap enough that a single developer can now build something that would've required a funded team in 2020.

The open-source services market is also in a prolonged growth moment. Multiple research firms peg it growing at roughly 15–17% CAGR through the late 2020s and into the 2030s, with estimates ranging from around $18 billion to over $35 billion in current market size (the spread tells you something about how loosely "open source services" gets defined, but the direction is consistent). The self-hostable, privacy-first positioning Kollect leans into isn't just ideological — it's increasingly a real procurement consideration, especially in Europe and in enterprise contexts where data residency matters. Building open-source-first is a go-to-market strategy now, not just a philosophical stance.

The question is whether voice-first surveys are a vitamin or a painkiller. The completion rate argument suggests painkiller — but we've been told that before.

## The Micro: What Kollect Actually Does (and the Bets It's Making)

Kollect's core mechanic is straightforward: instead of presenting a user with a static form, it initiates a voice conversation. The AI listens to spoken responses, adapts follow-up questions dynamically based on what it hears, and guides the respondent through the survey like a (hopefully non-annoying) interviewer would. The claim on their site is up to 3x better completion rates than traditional forms — which is a big number that needs more than a landing page to validate, but the directional logic isn't crazy. Talking is lower friction than typing for most people in most contexts.

On the builder side, you can either drag-and-drop standard block types — short answer, multiple choice, email, date, number — or describe what you want in plain language and let the AI generate the form structure. That second path is the more interesting one. If it works reliably, it collapses the distance between "I have a research question" and "I have a deployed survey" significantly.

The technical stack is TypeScript, which is a sane choice for a project that wants external contributors — type safety makes open-source codebases dramatically easier to onboard into. The GitHub repo is public (MIT license), and the product is self-hostable on whatever infrastructure you prefer: your own server, AWS, GCP, Azure. The managed cloud option exists for people who don't want to run their own stack, with the caveat that it's currently a "limited demo."

Product Hunt numbers: 129 upvotes, 13 comments, landed at #6 for the day. That's a respectable indie launch — not a breakout, but enough signal that the concept resonates with the PH audience, which skews toward developers and early-adopter product people. Thirteen comments is thin, though. That's a community that looked, nodded, and mostly moved on — engagement depth matters for gauging genuine enthusiasm versus passive interest.

The Deepgram dependency for voice processing is worth noting. It's not a knock — Deepgram is solid infrastructure — but it does mean there's a cost structure baked into the voice feature that self-hosters need to think about.

## The Verdict

Kollect is a well-reasoned bet on two things that are probably both true: forms are genuinely bad UX, and voice interfaces have finally crossed a usability threshold where they're not embarrassing to deploy. The open-source, self-hostable positioning is smart differentiation in a market where data privacy concerns are real and growing.

What we'd want to know before getting excited: Does the adaptive AI actually produce better-quality responses, or just more responses? Completion rate is a metric; data quality is the actual goal. The 3x claim needs a source. We'd also want to see the managed cloud mature — the "limited demo" framing suggests this is still early enough that production use cases are going to hit rough edges.

At 30 days, the signal to watch is GitHub stars and whether any non-trivial organizations self-host it. At 60 days, does the AI form generation hold up across diverse use cases or does it work great for customer feedback and fall apart for anything more structured? At 90 days, is there a community forming around the repo, or is this a solo project that launched well?

This is genuinely interesting work for what appears to be a small team (possibly one person). The ceiling is real. So is the execution risk.
